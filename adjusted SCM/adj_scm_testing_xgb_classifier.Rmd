---
title: "Adjusted SCM Model Testing"
output: html_notebook
author: Brian K. Masinde
---

```{r}
library(rpart)
library(caret)
library(pROC) # For AUC calculation
library(dplyr)
library(data.table)
library(mlflow)
library(purrr)
library(here)
```

# Inputs
Importing test data and models

```{r}
base_test <- read.csv(here("data", "base_test.csv"))

nrow(base_test)
```

```{r}
# Define the list of model names
model_names <- c("clas_full",
                 "wind",
                 "rain",
                 "roof_strong_wall_strong",
                 "roof_strong_wall_light",
                 "roof_strong_wall_salv",
                 "roof_light_wall_strong",
                 "roof_light_wall_light",
                 "roof_light_wall_salv",
                 "roof_salv_wall_strong",
                 "roof_salv_wall_light",
                 "roof_salv_wall_salv"
                )

# Create a named list to store the models
models_list <- list()

# Loop over each model name to construct the file path and read the RDS file
path <- here("adjusted SCM")
for (model_name in model_names) {
  # Construct the file path for the model
  file_path <- file.path(path, paste0("base_", model_name, "_model.rds"))

  # Read the model and store it in the list with the model name as the key
  models_list[[paste0("base_", model_name, "_model")]] <- readRDS(file_path)
}
```

```{r}
# Define models in a named list
col_models_list <- list(
  wind_max = models_list[["base_wind_model"]],
  rain_total = models_list[["base_rain_model"]],
  roof_strong_wall_strong = models_list[["base_roof_strong_wall_strong_model"]],
  roof_strong_wall_light = models_list[["base_roof_strong_wall_light_model"]],
  roof_strong_wall_salv = models_list[["base_roof_strong_wall_salv_model"]],
  roof_light_wall_strong = models_list[["base_roof_light_wall_strong_model"]],
  roof_light_wall_light = models_list[["base_roof_light_wall_light_model"]],
  roof_light_wall_salv = models_list[["base_roof_light_wall_salv_model"]],
  roof_salv_wall_strong = models_list[["base_roof_salv_wall_strong_model"]],
  roof_salv_wall_light = models_list[["base_roof_salv_wall_light_model"]],
  roof_salv_wall_salv = models_list[["base_roof_salv_wall_salv_model"]]
)
```


```{r}
df_base_test <-  base_test %>%
  mutate(across(names(col_models_list), ~ predict(col_models_list[[cur_column()]],
                                             newdata = base_test), .names = "{.col}_pred"))
```

```{r}
# Define wind and rain interaction variables
wind_fractions <- c("blue_ss_frac", "yellow_ss_frac", "orange_ss_frac", "red_ss_frac")
rain_fractions <- c("blue_ls_frac", "yellow_ls_frac", "orange_ls_frac", "red_ls_frac")

# Compute wind interaction terms dynamically
for (col in wind_fractions) {
  print(col)
  new_col_name <- paste0("wind_", col)
  df_base_test[[new_col_name]] <- df_base_test[[col]] * df_base_test[["wind_max_pred"]]
}

# Multiply rain fractions by rain_total_pred
for (col in rain_fractions) {
  new_col_name <- paste0("rain_", col)
  df_base_test[[new_col_name]] <- df_base_test[[col]] * df_base_test[["rain_total_pred"]]
}
```
```{r}
# We will need this for metrics comparison
df_base_test$damage_binary_2 <- factor(df_base_test$damage_binary,
                                       levels = c("0", "1"),  # Your current levels
                                       labels = c("Damage_below_10", "Damage_above_10"))  # New valid labels
```


```{r}
# predict for damage_binary
# Make probability predictions for classification
y_preds_probs <- predict(models_list[["base_clas_full_model"]], newdata = df_base_test, type = "prob")[,2]  # Probability of class 1
```

```{r}
# AUC
# Compute AUC (better for classification)
auc_value <- auc(roc(df_base_test$damage_binary_2, y_preds_probs))
auc_value
```

```{r}
# extracting probability that y_pred == 1
#y_preds_prob_1 <- y_preds_prob[ ,2]

## assigning final class based on threshold
y_pred <- ifelse(y_preds_probs > 0.35, 1, 0)

y_pred  <- factor(y_pred, levels = c("0", "1"),  # Your current levels
                                       labels = c("Damage_below_10", "Damage_above_10"))  # New valid labels
# using table function
conf_matrix <- confusionMatrix(as.factor(y_pred),
                     df_base_test$damage_binary_2,
                     positive = "Damage_above_10"
                     )
print(conf_matrix)

```
```{r}
# Positive class = 1, precision, recall, and F1
# Extract precision, recall, and F1 score
precision <- conf_matrix$byClass['Precision']
recall <- conf_matrix$byClass['Recall']
f1_score <- conf_matrix$byClass['F1']

cat("precision is:", sep = " ", precision, "\n")
cat("recall is:", sep = " ", recall, "\n")
cat("f1_score is:", sep = " ", f1_score, "\n")
```

```{r}
df_base_test$island_groups <- as.factor(df_base_test$island_groups)

# Loop through each group and generate a confusion matrix
for (grp in levels(df_base_test$island_groups)) {
  
  # Subset data for the current group
  group_indices <- df_base_test$island_groups == grp
  y_true_group <- df_base_test$damage_binary_2[group_indices]
  y_pred_group <- y_pred[group_indices]
  
  # Generate and print confusion matrix
  cat("Confusion Matrix for Island Group:", grp, "\n")
  print(confusionMatrix(y_pred_group, y_true_group, positive = "Damage_above_10"))
  cat("\n")
}
```


```{r}
models_list[["base_clas_full_model"]]$bestTune
```

```{r}
# logging in mflow:
# Logging the model and parameter using MLflow

# set tracking URI
mlflow_set_tracking_uri("http://127.0.0.1:5000")

# Ensure any active run is ended
suppressWarnings(try(mlflow_end_run(), silent = TRUE))

# set experiment
# Logging metrics for model training and the parameters used
mlflow_set_experiment(experiment_name = "SCM - XGBOOST classification -CV (Test metircs)")

# Ensure that MLflow has only one run. Start MLflow run once.
run_name <- paste("XGBoost Run", Sys.time())  # Unique name using current time


# Start MLflow run
mlflow_start_run(nested = FALSE)

# Ensure the run ends even if an error occurs
#on.exit(mlflow_end_run(), add = TRUE)

# Extract the best parameters (remove AUC column)
#best_params_model <- best_params %>% # Remove AUC column if present
#    select(-AUC)

parameters_used  <- models_list[["base_clas_full_model"]]$bestTune

# Log each of the best parameters in MLflow
for (param in names(parameters_used)) {
  mlflow_log_param(param, parameters_used[[param]])
}

# Log the model type as a parameter
mlflow_log_param("model_type", "scm-xgboost-classification")

# predicting
y_preds_probs <- predict(models_list[["base_clas_full_model"]], newdata = df_base_test, type = "prob")[,2]  # Probability of class 1
y_pred <- ifelse(y_preds_probs > 0.35, 1, 0)

y_pred  <- factor(y_pred, levels = c("0", "1"),  # Your current levels
                                       labels = c("Damage_below_10", "Damage_above_10"))  # New valid labels

# summarize results
conf_matrix <- confusionMatrix(as.factor(y_pred),
                     df_base_test$damage_binary_2,
                     positive = "Damage_above_10"
                     )

# accuracy
accuracy  <- conf_matrix$overall['Accuracy']

# Positive class = 1, precision, recall, and F1
# Extract precision, recall, and F1 score
precision <- conf_matrix$byClass['Precision']
recall <- conf_matrix$byClass['Recall']
f1_score <- conf_matrix$byClass['F1']
auc_value <- auc(roc(df_base_test$damage_binary_2, y_preds_probs))


# Log parameters and metrics
# mlflow_log_param("model_type", "scm-xgboost-classification")
mlflow_log_metric("accuracy", accuracy)
mlflow_log_metric("F1", f1_score)
mlflow_log_metric("Precision", precision)
mlflow_log_metric("Recall", recall)
#mlflow_log_metric("AUC", auc_value)


# Save model
#saveRDS(model, file = file.path(path_2_folder, "spam_clas_model.rds"))

# End MLflow run
mlflow_end_run()
```

